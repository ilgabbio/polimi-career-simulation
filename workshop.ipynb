{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the 2024 workshop!\n",
    "\n",
    "Read carefully the instructions throughout this notebook. They will guide you in the realm of industry-level\n",
    "anomaly detection problem. It will be instructive and (hopefully!) fun.\n",
    "\n",
    "The initial cells are fully working and ready to be run. Try to understand what each cell does, comments can\n",
    "help you during the process. Later on, there will be some parts that will require your intervention to work as\n",
    "expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StO4soRXgK3n",
    "outputId": "3dec16d2-9c92-4dee-b4b9-1ccbae90892a"
   },
   "outputs": [],
   "source": [
    "# This cell can be ignored. It will be required in case you need to setup an environment from scratch\n",
    "# (e.g. if you are using Colab).\n",
    "\n",
    "# !pip install anomalib imgaug kornia lightning matplotlib 'numpy<=2.0.0' onnxruntime opencv-python pandas pillow scikit-learn torch torchvision timm FrEIA open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the \"imports\" for the notebook. You could see some warnings, but they are safe to ignore.\n",
    "Simply run this cell and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCLxW2idhcbw",
    "outputId": "bf13ea96-2d7d-4046-83db-24222e7364ff"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from anomalib import TaskType\n",
    "from anomalib.data import BTech, MVTec\n",
    "from anomalib.data.utils import read_image\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.metrics import F1AdaptiveThreshold, ManualThreshold\n",
    "from anomalib.metrics.precision_recall_curve import BinaryPrecisionRecallCurve\n",
    "from anomalib.models import Padim, Fastflow, Patchcore, Uflow\n",
    "from anomalib.utils.post_processing import superimpose_anomaly_map, anomaly_map_to_color_map\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchmetrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYBR4D2IuU29"
   },
   "source": [
    "## Dataset\n",
    "The dataset we are going to work with is the so called \"MVTec\", which is the de facto battlefield for anomaly detection algorithms. The dataset is composed of several product categories coming from the industry.\n",
    "\n",
    "Reference: https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "\n",
    "Optionally, BTech dataset (released by _beanTech_ and University of Udine) could also be used.\n",
    "\n",
    "Reference https://ar5iv.labs.arxiv.org/html/2104.10036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dataset and their splits. In particular, it will download the whole dataset, it will extract and convert all the images and it will split the dataset in 3 distict groups (also called splits): _training_, _validation_ and _test_. It might require some time to complete. At the end you should see a \"datasets\" folder in your workspace directory with all the splits well organized.\n",
    "\n",
    "In case you already have a preprocessed version of the MVTec dataset, copy it in `./dataset` folder. In this way you will avoid all the download and processin phase. At the end, you should have a similar file structure:\n",
    "```\n",
    "./dataset\n",
    "  |-- MVTec\n",
    "  :     |-- bottle\n",
    "        |     |-- ground_truth\n",
    "        |     |-- train\n",
    "        |     `-- test\n",
    "        |-- cable\n",
    "        :     :\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YclK_lhTiBRl",
    "outputId": "0b4f3ecf-e6d2-40fd-ecd6-0d82b6ddc461"
   },
   "outputs": [],
   "source": [
    "L.seed_everything(321)\n",
    "\n",
    "datamodule = MVTec(\n",
    "    category=\"cable\",  # <--- We select only one category. Later you can replace this with others.\n",
    "    image_size=256,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    val_split_ratio=0.3,\n",
    "    val_split_mode='from_test',\n",
    "    test_split_ratio=0.3,\n",
    ")\n",
    "datamodule.prepare_data()  # <--- This line will download data, if necessary.\n",
    "datamodule.setup()\n",
    "\n",
    "# Ref: if you want to learn a bit more of the logic under the hood\n",
    "# see here https://github.com/openvinotoolkit/anomalib/blob/2c2fac1a32cb2101262971a1991d625d15e61005/src/anomalib/data/base/datamodule.py#L51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uyS7JwsWQv3"
   },
   "source": [
    "### Dataset statistics\n",
    "\n",
    "Dataset has been split into train/validation/test sets. Every sample in each\n",
    "set is essentially composed of an RGB image and a binary mask (its label).\n",
    "\n",
    "How many samples has dataset? How many of them are normal or defects?\n",
    "\n",
    "Let's find out, by printing few statistics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVAl3wa0WQOp",
    "outputId": "8a3c1597-8708-400f-fbe7-13172d3d4d50"
   },
   "outputs": [],
   "source": [
    "# Each 'sample' is a dictionary with the following keys: 'image_path', 'label', 'image', 'mask'.\n",
    "# Label is `1` for anomalies and `0` for normal samples. 'image' and 'mask' are numpy arrays.\n",
    "\n",
    "counter = 0\n",
    "for sample_id in range(len(datamodule.train_data)):\n",
    "    sample = datamodule.train_data[sample_id]\n",
    "    if sample[\"label\"] == 1:\n",
    "        counter += 1\n",
    "print(f\"Training set: {counter}/{sample_id + 1} anomalous samples\")\n",
    "\n",
    "counter = 0\n",
    "for sample_id in range(len(datamodule.val_data)):\n",
    "    sample = datamodule.val_data[sample_id]\n",
    "    if sample[\"label\"] == 1:\n",
    "        counter += 1\n",
    "print(f\"Validation set: {counter}/{sample_id + 1} anomalous samples\")\n",
    "\n",
    "counter = 0\n",
    "for sample_id in range(len(datamodule.test_data)):\n",
    "    sample = datamodule.test_data[sample_id]\n",
    "    if sample[\"label\"] == 1:\n",
    "        counter += 1\n",
    "print(f\"Test set: {counter}/{sample_id + 1} anomalous samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training set does *not* include anomalous samples. This is quite common in industry: defects\n",
    "are usually rare, and hence hard to predict. In this exercise we can only count on the few anomalous samples\n",
    "from the validation set to calibrate our model before testing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data!\n",
    "Whenever it is possible, it is strongly recommended to look at the data with your own eyes in order to get a feeling\n",
    "of the problem. The following functions will help you on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqmae33jqNbJ"
   },
   "outputs": [],
   "source": [
    "# The following functions will be used to plot image and label side by side.\n",
    "# This cell can be run as it is.\n",
    "\n",
    "def load_image_and_resize(image_path: str, size: Tuple[int, int]) -> NDArray:\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    height, width = size\n",
    "    return cv2.resize(image, (width, height))\n",
    "\n",
    "def show_image_and_mask(datamodule: L.LightningDataModule, index: int | None = None, split: str = \"train\"):\n",
    "    if split.startswith(\"train\"):\n",
    "        dataset = datamodule.train_data\n",
    "    elif split.startswith(\"val\"):\n",
    "        dataset = datamodule.val_data\n",
    "    elif split.startswith(\"test\"):\n",
    "        dataset = datamodule.test_data\n",
    "    else:\n",
    "        raise ValueError(\"Unknown split\")\n",
    "    \n",
    "    if index is None:\n",
    "        index = random.choice(range(len(dataset)))\n",
    "    sample = dataset[index]\n",
    "    gt = sample[\"mask\"]\n",
    "    im = load_image_and_resize(sample[\"image_path\"], gt.shape[-2:])\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(sample[\"image_path\"], fontsize=10)\n",
    "    ax[0].imshow(im)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"image\")\n",
    "    ax[1].imshow(gt, interpolation='nearest')\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar9n292OdoJC"
   },
   "source": [
    "Get an idea of the dataset. Plot a random sample from the training set. Run the following cell multiple times to pick other \"(_image_, _label_)\" pairs. As you can see the training set is composed of normal samples only (i.e. label full o 0's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "DnE2hOQeZOp7",
    "outputId": "5e828513-0180-492f-96f3-fc775b789df7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "show_image_and_mask(datamodule, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLyWfL6PhY3C"
   },
   "source": [
    "### EXERCISE\n",
    "Run the previous cell by changing dataset split (e.g. \"val\"). Run it multiple times. Since the number of anomalous samples\n",
    "is quite limited you could call the function `show_image_and_mask(...)` with the extra argument `index=XXX` in order to force\n",
    "a specific sample id. Try to print the IDs of the anomalous samples of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here few hints:\n",
    "#  - Look at the function `show_image_and_mask` and see how it extracts the \"sample\" object.\n",
    "#  - Try to iterate over the validation set keeping track of the IDs\n",
    "#  - The mask of anomalous samples will have pixel greater than 0.\n",
    "\n",
    "anomalous_ids = []\n",
    "for sample_id in range(len(datamodule.val_data)):\n",
    "    sample = datamodule.val_data[sample_id]\n",
    "    ### Fill this part...\n",
    "    # ...\n",
    "    # ...\n",
    "\n",
    "%matplotlib inline\n",
    "show_image_and_mask(datamodule, index=random.choice(anomalous_ids), split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model (for example Padim) and fit it on the training set.\n",
    "\n",
    "Start by defining few classic metrics. They will be measured both image-wise and pixel-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metrics = {\n",
    "    \"F1Score\": {\n",
    "        \"class_path\": \"torchmetrics.classification.BinaryF1Score\",\n",
    "        \"init_args\": {},\n",
    "    },\n",
    "    \"Precision\": {\n",
    "        \"class_path\": \"torchmetrics.classification.BinaryPrecision\",\n",
    "        \"init_args\": {},\n",
    "    },\n",
    "    \"Recall\": {\n",
    "        \"class_path\": \"torchmetrics.classification.BinaryRecall\",\n",
    "        \"init_args\": {},\n",
    "    },\n",
    "}\n",
    "pixel_metrics = image_metrics | {\"PerRegionOverlap\": {\"class_path\": \"anomalib.metrics.PRO\", \"init_args\": {}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are defining two important objects: the `model` (_Padim_ in this case) and the `engine`. The former is responsible of defining how features are extracted and classified, the latter is more a \"runner\" that invokes the model with the specific dataset and knows how to collect the results. Let's see them in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple configuration for Padim. Later in this workshop we will try to change this a bit.\n",
    "model = Padim(layers=[\"layer1\"], n_features=64)\n",
    "\n",
    "# For the `Engine` is enough to specify our metrics while leaving all the other parameters as default.\n",
    "engine = Engine(\n",
    "    image_metrics=image_metrics,\n",
    "    pixel_metrics=pixel_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training process is called `fit`. In particular, a fitting loop is composed of a _training phase_ followed by a _validation phase_. Notice that we are passing the `datamodule` object and let the `engine` selecting for the required split in each sub-phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inspect some predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_prediction(predictions: List[Dict], index: int) -> None:\n",
    "    batch_size = len(predictions[0]['image'])\n",
    "    batch_idx = index // batch_size\n",
    "    sample_idx = index % batch_size\n",
    "    \n",
    "    image = load_image_and_resize(\n",
    "        image_path=predictions[batch_idx][\"image_path\"][sample_idx],\n",
    "        size=predictions[batch_idx][\"anomaly_maps\"][sample_idx].shape[-2:],\n",
    "    )\n",
    "    anomaly_map = predictions[batch_idx][\"anomaly_maps\"][sample_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    heat_map = superimpose_anomaly_map(anomaly_map=anomaly_map, image=image, normalize=True)\n",
    "    pred_mask = predictions[batch_idx][\"pred_masks\"][sample_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    fig.suptitle(predictions[batch_idx][\"image_path\"][sample_idx], fontsize=10)\n",
    "    ax[0, 0].imshow(image)\n",
    "    ax[0, 0].axis(\"off\")\n",
    "    ax[0, 0].set_title(\"image\")\n",
    "    ax[0, 1].imshow(anomaly_map)\n",
    "    ax[0, 1].axis(\"off\")\n",
    "    ax[0, 1].set_title(\"anomaly map\")\n",
    "    ax[1, 0].imshow(heat_map)\n",
    "    ax[1, 0].axis(\"off\")\n",
    "    ax[1, 0].set_title(\"heatmap\")\n",
    "    ax[1, 1].imshow(pred_mask, interpolation='nearest')\n",
    "    ax[1, 1].axis(\"off\")\n",
    "    ax[1, 1].set_title(\"predicted mask\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: `engine.predict()` will use the 'test' split from the datamodule, unless specified explicitly.\n",
    "# Differently from the training and validation phase, `predict` does not compute metrics, but it will\n",
    "# return more elaborated outputs.\n",
    "predictions = engine.predict(model=model, datamodule=datamodule, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_single_prediction(predictions, index=1)  # <-- Try to change the index and explore other outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: `engine.test()` computes (and prints) the metrics.   \n",
    "test_results = engine.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase the depth of the backbone\n",
    "Features are extracted from a pretrained backbone (in this case a Resnet18). As you may know, deep networks are composed of series of layers and each layer highlights a specific semanatic. We don't know in advance which of them work better in our problem, but we have the tools to find out.\n",
    "\n",
    "In the following experiments we will classify the features collected at different layers and we will plot the respective metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "layer_groups = [[\"layer1\"], [\"layer2\"], [\"layer3\"]]\n",
    "\n",
    "# Fix the number of features (64) and increase the depth\n",
    "L.seed_everything(42)\n",
    "for layers in layer_groups:\n",
    "    model = Padim(layers=layers, n_features=64)\n",
    "    engine = Engine(\n",
    "        image_metrics=image_metrics,\n",
    "        pixel_metrics=pixel_metrics,\n",
    "    )\n",
    "    engine.fit(model=model, datamodule=datamodule)\n",
    "    results_i = engine.test(model=model, datamodule=datamodule)\n",
    "    for metric_name, metric_value in results_i[0].items():\n",
    "        results[metric_name].append(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marker_style(metric: str) -> str:\n",
    "    if \"image\" in metric:\n",
    "        color = \"r\"\n",
    "    elif \"pixel\" in metric:\n",
    "        color = \"b\"\n",
    "    else:\n",
    "        color = \"g\"\n",
    "    if \"F1\" in metric:\n",
    "        shape = \"s\"\n",
    "    elif \"Precision\" in metric:\n",
    "        shape = \"o\"\n",
    "    elif \"Recall\" in metric:\n",
    "        shape = \"^\"\n",
    "    else:\n",
    "        shape = \"*\"\n",
    "    return f\"{color}{shape}--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "xticks = np.arange(1, len(list(results.values())[0]) + 1) - 0.5\n",
    "for metric, result in results.items():\n",
    "    ax.plot(xticks, result, get_marker_style(metric),label=metric)\n",
    "ax.axis((0.3, 4, 0., 1.1))\n",
    "ax.set_title('Backbone depth')\n",
    "ax.set_xticks(ticks=xticks, labels=[f\"{layer_group}\" for layer_group in layer_groups])\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "It does not seem to be a \"clear winner\" between features from _Layer1_, _Layer2_ or _Layer3_.\n",
    "\n",
    "Try to run another experiment and use a subset of features extracted from both layers. Note that the argument `layers` takes a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase the number of features\n",
    "\n",
    "The features extracted from the backbone are (randomly) reduced in number. We could argue that the more retained features, the higher the metrics. Let's verify this argument with a simple experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = L.seed_everything(42)  # this is required for reproducibility of the random projection.\n",
    "\n",
    "n_features_sweep = [16, 32, 64, 128]\n",
    "for n_features in n_features_sweep:\n",
    "    model = Padim(layers=[\"layer1\", \"layer2\"], n_features=n_features)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Try to change the seed and check if results are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the threshold\n",
    "\n",
    "You might wonder how the threshold on the anomaly maps is chosen in order to produce a binary anomaly mask. By default, the _Engine_ selects the value that maximises the F1Score. This is how the validation set is used.\n",
    "\n",
    "Let's check how well the model behaves on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model, we'll keep this configuration throughout this section.\n",
    "engine = Engine(image_metrics=image_metrics, pixel_metrics=[])\n",
    "model = Padim(layers=[\"layer1\"], n_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test the fitted model against the validation set. Instead of feeding the `datamodule`,\n",
    "# pass the validation dataloader explicitly.\n",
    "_ = engine.test(model=model, dataloaders=[datamodule.test_dataloader()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've used the default settings for the dataset and the model you will notice an image-wise recall of **~92%**, which translates into 2 missed anomalous samples (i.e. false-negatives) out of the 27 included in the validation set.\n",
    "\n",
    "In the real world, depending on use cases this choice might not be ideal. For instance, sometimes you prefer to sacrifice false-positives in order to reduce the number of missed anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Find the (highest) threshold for which the image-wise recall on the validation set > 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea:\n",
    "#  - let's use a PR curve in order to take the threshold that satisfies the requirement.\n",
    "#  - Write a custom `Threshold` policy\n",
    "\n",
    "# You can start from this skeleton class and fill the missing part\n",
    "\n",
    "class MyThreshold(BinaryPrecisionRecallCurve, ManualThreshold):\n",
    "    def __init__(self, default_value: float = 0.5, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_state(\"value\", default=torch.tensor(default_value), persistent=True)\n",
    "        self.value = torch.tensor(default_value)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute the threshold.\n",
    "        \n",
    "        Store the threshold into `self.value` and return it\n",
    "        \"\"\"\n",
    "        precision: torch.Tensor\n",
    "        recall: torch.Tensor\n",
    "        thresholds: torch.Tensor\n",
    "        precision, recall, thresholds = super().compute()  # <-- use these!\n",
    "\n",
    "        # FILL HERE:\n",
    "        # ...\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{super().__repr__()} (value={self.value:.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify custom threshold, pass it explicitly to the `Engine`.\n",
    "engine = Engine(\n",
    "    threshold=MyThreshold(),\n",
    "    image_metrics=image_metrics,\n",
    "    pixel_metrics=[],\n",
    ")\n",
    "model = Padim(layers=[\"layer1\"], n_features=64)\n",
    "engine.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val = engine.test(model=model, dataloaders=[datamodule.val_dataloader()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how metrics have changed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = engine.test(model=model, dataloaders=[datamodule.test_dataloader()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry picking\n",
    "\n",
    "As already stated above, it is very important to always pair numbers with visual inspection. A common practice in data-science is to manually select few examples where the model behaves particularly well or where it struggles the most. This process is called \"[_cherry-picking_](https://en.wikipedia.org/wiki/Cherry_picking)\".\n",
    "\n",
    "### EXERCISE\n",
    "Try to identify the missed anomalous samples from the test set and plot their anomaly maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the model: what about PatchCore?\n",
    "\n",
    "Reference: https://www.amazon.science/publications/towards-total-recall-in-industrial-anomaly-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `Patchcore` subclasses `AnomalyModule`, like `Padim` and can be fed into an `Engine\n",
    "pc = Patchcore(backbone=\"resnet18\", layers=[\"layer3\"], coreset_sampling_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Try with different size of the \"coreset\" (10% and 1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the model: what about FastFlow?\n",
    "\n",
    "Reference: https://arxiv.org/pdf/2111.07677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Engine(\n",
    "    image_metrics=image_metrics,\n",
    "    pixel_metrics=pixel_metrics,\n",
    "    max_epochs=1,\n",
    "    check_val_every_n_epoch=None,\n",
    ")\n",
    "ff = Fastflow()\n",
    "engine.fit(model=ff, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = engine.test(model=ff, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the _engine_ has been configured to fit for one \"epoch\" only. It means that model parameters has been \"adjusted\" of a tiny amount to match the training set only once (actually, for a number of steps that fed the training samples only once). In this case, the pretrained model used as a starting point was sufficiently general purpose to be adequate enough for our dataset. However, we could constrain the model to adhere (i.e. _fit_) our training dataset even more (hence trading its generality in for some specialization). This can be obtained by extending the training time with more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Increment the number of number of epochs (up to 10, 20 or even more). Don't raise this value too much since it will require some time to complete. After each experiment, keep track of the metrics in a dictionary and plot them a graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Take the model that has the highest recall image-wise and check which are the hardest samples to identify by plotting their anomaly mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are all those examples really necessary?\n",
    "\n",
    "Let's see what happens when the number of training samples is progressively reduced.\n",
    "\n",
    "### EXERCISE\n",
    "Plot Precision/Recall/F1Score metrics produced by few experiemnts with (even aggressively) reduced training set. Use FastFlow (default settings) as a model and train each experiment for 5 epochs.\n",
    "\n",
    "How do you interpret the results? Why the precision is so low compared to the recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: the Engine can take extra arguments from the PyTorch Lightning library \n",
    "# (see here for reference: https://lightning.ai/docs/pytorch/stable/common/trainer.html#limit-train-batches)\n",
    "\n",
    "L.seed_everything(3210)\n",
    "training_set_fractions = [0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "\n",
    "# FILL HERE\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "As you can see from plots above, having more data can help. However, this is not always possible (either because data do not exist at all or simply because collecting more samples is too expensive). One possibility to mitigate this issue is to \"artifically\" augment the samples using \"augmentation strategies\". For example, in our scenario a 90-degree rotated sample is still a valid normal sample and hence it can be added to the training set. As you can easily imagine we could increase the dataset size of a factor of four, by simply adding rotated examples. The same goes for light color corrections (ligher/darker images), geometric transformation etc. As long as your transformed samples are normal you're safe to go.\n",
    "\n",
    "Pay attention that in some cases augmentations could produce anomalous samples (maybe a rotated cable is considered a defect, this is dependent to the problem definitions/requirements) and they should be avoided.\n",
    "\n",
    "Until now, default augmentations have been used. These are somehow imposed by the pretrained models used throughout this notebook. However, they can be enriched with extra operations. Augmentations are `torchvision.transforms.v2.Trasnform` objects that are stored as attributes in the dataset. In particular, default augmentations (used so far by for both train/val/test) are\n",
    "```python\n",
    "Compose(\n",
    "      Resize(size=[256], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
    ")\n",
    "```\n",
    "which means that samples are first downscaled (and potentially stretched) to a resolution of 256x256 and then normalized so that pixel values are normally distributed. Obviously, normalization (like also color adjustments) is not applied to labels, but this is handled internally by the dataset. In fact, only geometric transformations are propagated to label masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "\n",
    "Look at the [available augmentations](https://pytorch.org/vision/0.19/transforms.html#v2-api-reference-recommended) and extend the **training** augmentation pipeline. Before running other experiments try to inspect the effect of augmentation by plotting few examples. You might want to disable normalization (or apply a de-norm) in order to get proper color ranges.\n",
    "\n",
    "Next, take a reduced training set (for instance 10%) and try to improve the metrics by cooking a good training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "#  * have a look at how the `AnomalibDataModule` class definition;\n",
    "#  * actual datasets are initialized during the datamodule `.setup()`\n",
    "#  * transformation are applied during dataset `__getitem__()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore other approaches!\n",
    "State of the art techniques can get an image F1 Score > 92% on this dataset. Do you have an idea to get close to that value? Anomalib library provides [many models](https://github.com/openvinotoolkit/anomalib/tree/main/src/anomalib/models/image) that are ready-to-use. All of them can be plugged into the _engine_, fitted and tested on the dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06d2ee9965744e50956bfef495c8d411": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07bd11739cf24dcca7e39672201b9eab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "119f261df4324981a420248f42d2220e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "151451502aa442d38fb2c3d8ff71a1bd": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_8c7bb2bf92c54ade961197176f62461c",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/0 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">13/13</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:06:14 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.08it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 0/0 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m13/13\u001b[0m \u001b[38;5;245m0:06:14 • 0:00:00\u001b[0m \u001b[38;5;249m0.08it/s\u001b[0m \u001b[37m \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "27db32ed5af54270aa42469b77d160f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32788af8dad3455da2423db1dc31284e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3636ab04055043a28e3a9be7b4723688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_119f261df4324981a420248f42d2220e",
      "placeholder": "​",
      "style": "IPY_MODEL_b1539630542245318e5ad301754fcf4a",
      "value": "model.safetensors: 100%"
     }
    },
    "4ad10b807cc047fa9912ee9dd3d80d0b": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_fb50ce4b43d34a76b2b7c96f9ffff85d",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3/3</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:45 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.07it/s</span>  \n</pre>\n",
         "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m3/3\u001b[0m \u001b[38;5;245m0:00:45 • 0:00:00\u001b[0m \u001b[38;5;249m0.07it/s\u001b[0m  \n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "5225e1c52b3f4001a8c2d40ddacc3803": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "591bacf7bae84d1a886839a9da903c05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "706fefcb8a664b09a4f11d48bc9ee739": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27db32ed5af54270aa42469b77d160f3",
      "max": 275835296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07bd11739cf24dcca7e39672201b9eab",
      "value": 275835296
     }
    },
    "7b023861c4aa4be9a59716eb7a10598e": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_32788af8dad3455da2423db1dc31284e",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 3/999 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3/13</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:09 • 0:00:33</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.31it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: -307109.438        </span>\n                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pixel_AUROC: 0.953 train_loss_epoch:</span>\n                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">-243063.719                         </span>\n</pre>\n",
         "text/plain": "Epoch 3/999 \u001b[38;2;98;6;224m━━━━━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m3/13\u001b[0m \u001b[38;5;245m0:00:09 • 0:00:33\u001b[0m \u001b[38;5;249m0.31it/s\u001b[0m \u001b[37mtrain_loss_step: -307109.438        \u001b[0m\n                                                                               \u001b[37mpixel_AUROC: 0.953 train_loss_epoch:\u001b[0m\n                                                                               \u001b[37m-243063.719                         \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "82ad7b84e7fb4655836169707f33bf7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84bf74d864fd489f8439c8856976c3c6": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_5225e1c52b3f4001a8c2d40ddacc3803",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">8/8</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:00 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.12it/s</span>  \n</pre>\n",
         "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m8/8\u001b[0m \u001b[38;5;245m0:01:00 • 0:00:00\u001b[0m \u001b[38;5;249m0.12it/s\u001b[0m  \n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "87dca79472cb4babb74773652606300f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_591bacf7bae84d1a886839a9da903c05",
      "placeholder": "​",
      "style": "IPY_MODEL_06d2ee9965744e50956bfef495c8d411",
      "value": " 276M/276M [00:01&lt;00:00, 175MB/s]"
     }
    },
    "8c7bb2bf92c54ade961197176f62461c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c9bf773bb6541f0aa509616ed9b0969": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_82ad7b84e7fb4655836169707f33bf7a",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/0 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">13/13</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:38 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.36it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> </span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 0/0 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m13/13\u001b[0m \u001b[38;5;245m0:00:38 • 0:00:00\u001b[0m \u001b[38;5;249m0.36it/s\u001b[0m \u001b[37m \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "b1539630542245318e5ad301754fcf4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b82d95d690f44322b6cc4933913c9ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3636ab04055043a28e3a9be7b4723688",
       "IPY_MODEL_706fefcb8a664b09a4f11d48bc9ee739",
       "IPY_MODEL_87dca79472cb4babb74773652606300f"
      ],
      "layout": "IPY_MODEL_f4fdf7df88e547bab0d49c107fbcf41f"
     }
    },
    "f4fdf7df88e547bab0d49c107fbcf41f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb50ce4b43d34a76b2b7c96f9ffff85d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
